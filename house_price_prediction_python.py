# -*- coding: utf-8 -*-
"""House_Price_Prediction_Python.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L3iM1kv2yRCykwMQeTTrKXPAMgydXS0B
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import seaborn as sns

df=pd.read_csv("/content/housing.csv")

df.head()

df.shape

df.info()

df.dropna(inplace=True)

df.info()

"""pandas removes any row that has even 1 NaN.

Since only the column total_bedrooms has missing values (20433 rows), pandas removes 20640 ‚àí 20433 = 207 rows.

So the remaining rows become 20433, and ALL columns show 20433 non-null rows.

üëâ This is normal behavior of dropna ‚Äî it drops the entire row, not just the NaN from one column.
"""

from sklearn.model_selection import train_test_split
X=df.drop(['median_house_value'],axis=1)
Y=df['median_house_value']

X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2)

train_data=X_train.join(Y_train)

"""üîç When is X_train.join(Y_train) used?

Only in EDA (Exploratory Data Analysis):

To calculate correlations

To check distributions

To plot data

To view the full training row (including target)

‚úî For analysis only
‚ùå Not for model training
"""

train_data

train_data.hist(figsize=(15,8))

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
train_data['ocean_proximity'] = le.fit_transform(train_data['ocean_proximity'])

plt.figure(figsize=(15,8))
sns.heatmap(train_data.corr(), annot=True, cmap="YlGnBu")

train_data['total_rooms']=np.log(train_data['total_rooms']+1)
train_data['total_bedrooms']=np.log(train_data['total_bedrooms']+1)
train_data['population']=np.log(train_data['population']+1)
train_data['households']=np.log(train_data['households']+1)
train_data.hist(figsize=(15,8))

train_data = train_data.join(pd.get_dummies(train_data['ocean_proximity'])).drop(['ocean_proximity'], axis=1)

train_data

plt.figure(figsize=(15,8))
sns.heatmap(train_data.corr(), annot=True, cmap="YlGnBu")

plt.figure(figsize=(15,8))
sns.scatterplot(x='latitude',y='longitude',data=train_data,hue='median_house_value',palette='coolwarm')

train_data['bedroom_ratio']=train_data['total_bedrooms']/train_data['total_rooms']
train_data['household_rooms']=train_data['total_rooms']/train_data['households']

plt.figure(figsize=(15,8))
sns.heatmap(train_data.corr(),annot=True,cmap='YlGnBu')

categorical_cols = X_train.select_dtypes(include=['object']).columns
print(categorical_cols)

train_data.columns = train_data.columns.astype(str)
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor

# Split features and target
X = train_data.drop('median_house_value', axis=1)
y = train_data['median_house_value']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Identify numeric + categorical
numeric_cols = X_train.select_dtypes(include=['int64','float64']).columns
categorical_cols = X_train.select_dtypes(include=['object']).columns

# Numeric pipeline
numeric_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# Categorical pipeline
categorical_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(handle_unknown='ignore'))
])

# Full preprocessing
preprocessor = ColumnTransformer([
    ('num', numeric_transformer, numeric_cols),
    ('cat', categorical_transformer, categorical_cols)
])

# Build final pipeline
model = Pipeline([
    ('preprocessor', preprocessor),
    ('regressor', RandomForestRegressor(random_state=42))
])

# Convert all column names to string
train_data.columns = train_data.columns.astype(str)

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

x_train = train_data.drop(['median_house_value'], axis=1)
y_train = train_data['median_house_value']
X_train_s=scaler.fit_transform(X_train)


reg = LinearRegression()
reg.fit(x_train, y_train)

LinearRegression()

test_data = X_test.join(Y_test)

test_data['total_rooms'] = np.log(test_data['total_rooms'] + 1)
test_data['total_bedrooms'] = np.log(test_data['total_bedrooms'] + 1)
test_data['population'] = np.log(test_data['population'] + 1)
test_data['households'] = np.log(test_data['households'] + 1)


test_data['bedroom_ratio'] = test_data['total_bedrooms'] / test_data['total_rooms']
test_data['household_rooms'] = test_data['total_rooms'] / test_data['households']
X_test, y_test = test_data.drop(['median_house_value'], axis=1), test_data['median_house_value']

X_test_s=scaler.transform(X_test)

x_train = train_data.drop(['median_house_value'], axis=1)
y_train = train_data['median_house_value']

# FIX COLUMN NAMES FIRST
train_data.columns = train_data.columns.astype(str)

# FIX INDEX
train_data = train_data.reset_index(drop=True)

# Split X and y from SAME dataset
X = train_data.drop('median_house_value', axis=1)
y = train_data['median_house_value']

# Now split ‚Äì THIS fixes the mismatch
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(X_train.shape, y_train.shape)

param_grid = {
    'regressor__n_estimators': [100, 200, 300],
    'regressor__min_samples_split': [2, 4],
    'regressor__max_depth': [None, 4, 8]
}

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor

train_data.columns = train_data.columns.astype(str)
train_data = train_data.reset_index(drop=True)

X = train_data.drop('median_house_value', axis=1)
y = train_data['median_house_value']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

numeric_cols = X_train.select_dtypes(include=['int64','float64']).columns
categorical_cols = X_train.select_dtypes(include=['object']).columns

numeric_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer([
    ('num', numeric_transformer, numeric_cols),
    ('cat', categorical_transformer, categorical_cols)
])

model = Pipeline([
    ('preprocessor', preprocessor),
    ('regressor', RandomForestRegressor(random_state=42))
])

param_grid = {
    'regressor__n_estimators': [100, 200, 300],
    'regressor__min_samples_split': [2, 4],
    'regressor__max_depth': [None, 4, 8]
}

grid_search = GridSearchCV(
    model,
    param_grid,
    cv=5,
    scoring='neg_mean_squared_error',
    return_train_score=True
)

grid_search.fit(X_train, y_train)

grid_search.fit(X_train, y_train)
print(grid_search.best_params_)

reg.score(X_test_s,y_test)

from sklearn.ensemble import RandomForestRegressor
forest=RandomForestRegressor()
forest.fit(X_train_s,Y_train)

forest.score(X_test_s,Y_test)

from sklearn.model_selection import GridSearchCV
param_grid ={
    "n_estimators":[3,10,30],
    "max_features":[2,4,6,8],
    "min_samples_split":[2,4,6,8]
}
grid_search=GridSearchCV(forest,param_grid,cv=5,scoring='neg_mean_squared_error',return_train_score=True)
grid_search.fit(X_train_s,Y_train)

best_forest=grid_search.best_estimator_

best_forest.score(X_test_s,Y_test)

print(X_train_s.shape)
print(y_train.shape)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_s = scaler.fit_transform(X_train)
X_test_s = scaler.transform(X_test)

X_train, X_test, y_train, y_test = train_test_split( X, Y , test_size=0.2, random_state=42)

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor
forest= RandomForestRegressor()

param_grid = {
    "n_estimators": [100, 200, 300],
    "min_samples_split": [2, 4],
    "max_depth": [None, 4, 8],
}

grid_search = GridSearchCV(
    forest,
    param_grid,
    cv=5,
    scoring="neg_mean_squared_error",
    return_train_score=True
)
grid_search.fit(x_train_s, y_train)

grid_search.best_estimator_
grid_search.best_estimator_.score(X_test_s,y_test)

